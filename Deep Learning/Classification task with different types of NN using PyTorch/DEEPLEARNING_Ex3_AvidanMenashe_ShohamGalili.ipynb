{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **imports**"
      ],
      "metadata": {
        "id": "E3iKzHYy4idF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pzEH62dstAva"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from copy import deepcopy\n",
        "from torchvision.models import mobilenet_v2\n",
        "from torchvision import models\n",
        "import sys\n",
        "from torchsummary import summary\n",
        "from torchvision.models import MobileNet_V2_Weights\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **global variables**"
      ],
      "metadata": {
        "id": "nEMFzDSy4vbo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YmWWgvamtMtQ"
      },
      "outputs": [],
      "source": [
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~global variables~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "batch_size = 512\n",
        "optimizer_type ='Adam' #'SGD' # 'RMSProp'\n",
        "momentum = 0.0\n",
        "#Flag indicating whether to use CUDA (GPU acceleration) if available\n",
        "cuda = True\n",
        "seed = 42\n",
        "# Set seed , every time you run the code, you will get the same random numbers generated from numpy.random and torch.random\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "#Checks if CUDA is available and the cuda flag is set to True\n",
        "use_cuda = cuda and torch.cuda.is_available()\n",
        "\n",
        "#if we are using CUDA, set the seed for the GPU as well so the random numbers generated on the GPU are the same as before with the same seed\n",
        "if use_cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Handel GPU stochasticity\n",
        "#to ensure that the results are reproducible and not random even on GPU, we need to set the following flags\n",
        "#there is no connection to the seed also if the seed is different, the results will be the same deterministic\n",
        "torch.backends.cudnn.enabled = use_cuda\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# use GPU if available\n",
        "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# Specifies the device to be used for computation (CPU or GPU).\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **loaders**"
      ],
      "metadata": {
        "id": "q8RjRwUX46dd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S4uLTVOAtWz3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    load the STL-10 dataset\n",
        "    :return: the trainloader ,testloader and the visualize loader of the dataset.\n",
        "             in addition, the transform for the trainset and the testset\n",
        "    \"\"\"\n",
        "    global batch_size\n",
        "    #TODO: how i know the size of normalization (mean and std) for the STL-10 dataset\n",
        "\n",
        "\n",
        "    transform_test = transforms.Compose([transforms.CenterCrop(size=(64,64)),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))])\n",
        "\n",
        "    #create transform for the visualization\n",
        "    transform_visualize = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    # create the transform for the trainset and testset\n",
        "    transform_train = transforms.Compose(\n",
        "        [transforms.RandomCrop(size=(64, 64)),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n",
        "         # data augmentation\n",
        "         transforms.RandomHorizontalFlip(p=0.5),\n",
        "         transforms.RandomRotation(degrees=8)\n",
        "         ])\n",
        "\n",
        "    #create the trainset and testset\n",
        "    trainset = torchvision.datasets.STL10(root='./data', split='train',download=True, transform=None)\n",
        "    testset = torchvision.datasets.STL10(root='./data', split='test', download=False, transform=transform_test)\n",
        "    visualize_set= torchvision.datasets.STL10(root='./data', split='train', download=False, transform=transform_visualize)\n",
        "\n",
        "    #create the differnet loaders\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=1)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=testset.data.shape[0],shuffle=False, num_workers=1)\n",
        "\n",
        "    visualize_loader = torch.utils.data.DataLoader(visualize_set, batch_size=batch_size,shuffle=True, num_workers=1)\n",
        "\n",
        "    return trainloader, testloader, visualize_loader,transform_train,transform_test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **visualize** **data**"
      ],
      "metadata": {
        "id": "oTITh4uh5BQK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "anA0EsJLtmJ3"
      },
      "outputs": [],
      "source": [
        "def visualize_data(dataset):\n",
        "    \"\"\"\n",
        "    visualize the data by show 4 images per class\n",
        "    :param dataset: the dataset we want to visualize\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    #TODO:ask ran if the we need to show image as 3X96X96 or 3X64X64\n",
        "\n",
        "    # STL10 classes\n",
        "    classes = ('airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck')\n",
        "\n",
        "    # Create a grid of 10x4\n",
        "    fig, axs = plt.subplots(10, 4, figsize=(15, 15))\n",
        "    #fig.tight_layout(pad=1.0)\n",
        "\n",
        "    # Loop over each class\n",
        "    for i, _class in enumerate(classes):\n",
        "        # Get the indexes of the images with the same class,each class has a list of indexes\n",
        "        idxs = [j for j, label in enumerate(dataset.labels) if label == i]\n",
        "        for j in range(4):\n",
        "            #get the image of the corresponding index\n",
        "            img = dataset[idxs[j]][0]\n",
        "            # unnormalize the image to [0,1] for visualization\n",
        "            #because the image is normalized to [-1,1] in the load_data function\n",
        "            img = img / 2 + 0.5\n",
        "            # convert the tensor to numpy for visualization\n",
        "            npimg = img.numpy()\n",
        "            # transpose the image to (H,W,C) from (C,H,W)\n",
        "            axs[i, j].imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "            axs[i, j].axis('off')\n",
        "\n",
        "        # Add the class name at the beginning of each row on the left side\n",
        "        axs[i, 0].text(-15, 15, classes[i], va='center', ha='right')\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **show agumentation**"
      ],
      "metadata": {
        "id": "xriPze095HvA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yFXZrztfwqCr"
      },
      "outputs": [],
      "source": [
        "def show_agumentation(dataset):\n",
        "    \"\"\"\n",
        "    this function will show the agumentation of the dataset\n",
        "    the first agumenation will be horizontal flip and the second will be rotation by 8 degrees\n",
        "    :param dataset:  the dataset we to show how the agumentation affect the images\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    #take a random image from the dataset\n",
        "    index = np.random.randint(0, len(dataset))\n",
        "    img = dataset[index][0]\n",
        "\n",
        "    #define the agumentation transformataion of horizontal flip\n",
        "    transform_horizontal_flip = transforms.Compose([transforms.RandomHorizontalFlip(p=1)])\n",
        "\n",
        "    #define the agumentation transformataion of rotation\n",
        "    transform_rotation = transforms.Compose([transforms.RandomRotation(degrees=8)])\n",
        "\n",
        "    #create the figure that will store the images\n",
        "    #the first image in each row will be the original image, and the second image will be the agumentation\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
        "\n",
        "    horizotal_img = transform_horizontal_flip(img)\n",
        "    rotation_img = transform_rotation(img)\n",
        "\n",
        "    #show the images\n",
        "    ax[0, 0].imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
        "    ax[0, 0].set_title('original image')\n",
        "    ax[0, 0].axis('off')\n",
        "    ax[0, 1].imshow(np.transpose(horizotal_img.numpy(), (1, 2, 0)))\n",
        "    ax[0, 1].set_title('horizontal flip')\n",
        "    ax[0, 1].axis('off')\n",
        "    ax[1, 0].imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
        "    ax[1, 0].set_title('original image')\n",
        "    ax[1, 0].axis('off')\n",
        "    ax[1, 1].imshow(np.transpose(rotation_img.numpy(), (1, 2, 0)))\n",
        "    ax[1, 1].set_title('rotation')\n",
        "    ax[1, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **split train and validation**"
      ],
      "metadata": {
        "id": "9PH10n7v5PFW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "diGWyHDutu4o"
      },
      "outputs": [],
      "source": [
        "def split_train_val(trainset,train_size=0.85):\n",
        "    \"\"\"\n",
        "    split the trainset to trainset and valset\n",
        "    :param trainset: the trainset we want to split\n",
        "    :param train_size: the size of the trainset in precentage\n",
        "    :return: the trainset and valset\n",
        "    \"\"\"\n",
        "    train_ind, valset_ind = torch.utils.data.random_split(trainset,[int(train_size * len(trainset)),len(trainset) - int(train_size * len(trainset))])\n",
        "\n",
        "    valset = deepcopy(trainset)\n",
        "    valset.data = valset.data[valset_ind.indices]\n",
        "    valset.labels = valset.labels[valset_ind.indices]\n",
        "    trainset.data = trainset.data[train_ind.indices]\n",
        "    trainset.labels = trainset.labels[train_ind.indices]\n",
        "\n",
        "    return trainset, valset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **train function**"
      ],
      "metadata": {
        "id": "qqkgERMz5c-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0CV00l6otz15"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, val_loader,loss_function, optimizer,num_of_epochs):\n",
        "    \"\"\"\n",
        "    this function will train the model\n",
        "    :param model: the model we want to train\n",
        "    :param data_loader: the data loader of the trainset\n",
        "    :param val_loader: the data loader of the valset\n",
        "    :param num_of_epochs: the number of the epochs we want to train the model\n",
        "    :return: the lost and accuarcy of the model during the validation and train set\n",
        "    \"\"\"\n",
        "    global device\n",
        "\n",
        "    #~~~~~~~~~~~~~~~~~~~~~~~~train~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    #move the model to the device\n",
        "    model.to(device)\n",
        "\n",
        "    #save the lost of the model during the validation and train set\n",
        "    # also save the accuracy\n",
        "    loss_train = []\n",
        "    loss_validation = []\n",
        "    accuracy_train = []\n",
        "    accuracy_validation = []\n",
        "    #best_accuracy = 0\n",
        "    #best_weights = self.weights\n",
        "\n",
        "    #go over the epochs\n",
        "    for epoch in range(num_of_epochs):\n",
        "        mini_batch_loss = []\n",
        "        mini_batch_accuracy = []\n",
        "        #go over the mini batches\n",
        "        for X_i, Y_i in data_loader:\n",
        "            #move the mini batch to the device\n",
        "            X_i, Y_i = X_i.to(device), Y_i.to(device)\n",
        "            #first do a forward pass to get the predictions\n",
        "            y_hat = model(X_i)\n",
        "            #calculate the loss of the mini batch\n",
        "            loss = loss_function(y_hat, Y_i)\n",
        "            #backpropagation pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            #calculate the accuracy of the mini batch\n",
        "            mini_batch_accuracy.append(torch.mean((Y_i == torch.argmax(y_hat,axis=1)).float()).item())\n",
        "            #add the lost of the mini batch to the list\n",
        "            mini_batch_loss.append(loss.item())\n",
        "\n",
        "        #calculate the lost and the accuracy of the current epoch\n",
        "        loss_train.append(np.mean(mini_batch_loss))\n",
        "        accuracy_train.append(np.mean(mini_batch_accuracy))\n",
        "\n",
        "        #~~~~~~~~~~~~~~~~~~~~~~~~validation~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "        #set the model to evaluation mode\n",
        "        model.eval()\n",
        "        #dont calculate the gradients\n",
        "        with torch.no_grad():\n",
        "            #get the validation set\n",
        "            X_validation, y_validation = next(iter(val_loader))\n",
        "            #move the validation set to the device\n",
        "            X_validation, y_validation = X_validation.to(device), y_validation.to(device)\n",
        "            #first do a forward pass to get the predictions\n",
        "            y_hat_val = model(X_validation)\n",
        "            #calculate the lost of the validation set\n",
        "            loss_val = loss_function(y_hat_val,y_validation)\n",
        "            #calculate the accuracy of the validation set\n",
        "            #mini_batch_accuracy.append(torch.mean((yi == torch.argmax(yhat)).float()).item())\n",
        "            accuracy_val=torch.mean((y_validation == torch.argmax(y_hat_val,axis=1)).float()).item()\n",
        "\n",
        "        #save the validation lost and accuracy of the validation set\n",
        "        loss_validation.append(loss_val.item())\n",
        "        accuracy_validation.append(accuracy_val)\n",
        "\n",
        "            #print the results of the current epoch\n",
        "        sys.stdout.write('\\r' + f'Epoch: {epoch + 1}')\n",
        "        sys.stdout.write(f'Train Loss: {loss_train[-1]:.3f} | '\n",
        "                         f'Train Accuracy: {100 * accuracy_train[-1]:.3f}% ')\n",
        "        sys.stdout.write(f'Validation Loss: {loss_validation[-1]:.3f} | '\n",
        "                         f'Validation Accuracy: {100 * accuracy_validation[-1]:.3f}%')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "    #return the lost and the accuracy of the model during the validation and train set\n",
        "    return loss_train,loss_validation,accuracy_train,accuracy_validation,model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **show plots**"
      ],
      "metadata": {
        "id": "msqX2OHo5k4_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b-L0p47-uCCV"
      },
      "outputs": [],
      "source": [
        "def show_plots(lost_train,lost_validation,accuracy_train,accuracy_validation):\n",
        "    \"\"\"\n",
        "    this function will show the plots of the lost and the accuracy\n",
        "    :param lost_train: the lost of the train\n",
        "    :param lost_validation: the lost of the validation\n",
        "    :param accuracy_train: the accuracy of the train\n",
        "    :param accuracy_validation: the accuracy of the validation\n",
        "    \"\"\"\n",
        "    #create a figure that will store the plots\n",
        "    fig, ax = plt.subplots(2, 1, figsize=(15, 15))\n",
        "    #show the lost plot\n",
        "    ax[0].plot(lost_train, label='train', linewidth=2,marker='o')\n",
        "    ax[0].plot(lost_validation, label='validation', linewidth=2,marker='v')\n",
        "    #give a title to the axis\n",
        "    ax[0].set_xlabel('epochs',fontsize=15)\n",
        "    ax[0].set_ylabel('lost',fontsize=15)\n",
        "    #give a title to the plot\n",
        "    ax[0].set_title('lost as a function of the epochs',fontsize=17)\n",
        "    ax[0].legend(fontsize=12)\n",
        "    #add grid\n",
        "    ax[0].grid(True)\n",
        "    #show the accuracy plot\n",
        "    ax[1].plot(accuracy_train, label='train',linewidth=2,marker='o')\n",
        "    ax[1].plot(accuracy_validation, label='validation',linewidth=2,marker='v')\n",
        "    #give a title to the axis\n",
        "    ax[1].set_xlabel('epochs',fontsize=15)\n",
        "    ax[1].set_ylabel('accuracy',fontsize=15)\n",
        "    #give a title to the plot\n",
        "    ax[1].set_title('accuracy as a function of the epochs',fontsize=17)\n",
        "    ax[1].legend(fontsize=12)\n",
        "    #add grid\n",
        "    ax[1].grid(True)\n",
        "    #adjust space between the plots\n",
        "    plt.subplots_adjust(hspace=0.4)\n",
        "    #show the plots\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **flatten logistic regression**"
      ],
      "metadata": {
        "id": "BwEKg7hR5poW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6-qpVj2nvDKe"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, num_of_classes, image_size=64*64*3):\n",
        "        \"\"\"\n",
        "        constructor\n",
        "        :param input_size: the input size\n",
        "        :param num_classes: the number of classes\n",
        "        \"\"\"\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        #flatten the image to a vector\n",
        "        self.flatten = nn.Flatten()\n",
        "        #linear layer\n",
        "        self.linear = nn.Linear(image_size, num_of_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward function\n",
        "        :param x: the input\n",
        "        :return: the output\n",
        "        \"\"\"\n",
        "        #flatten the image and pass it through the linear layer\n",
        "        predictions = self.flatten(x)\n",
        "        predictions = self.linear(predictions)\n",
        "\n",
        "        return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANN**"
      ],
      "metadata": {
        "id": "6FO7CFSJ5ySN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mk2PGfThv8ky"
      },
      "outputs": [],
      "source": [
        "class ANN(nn.Module):\n",
        "    def __init__(self, num_of_classes,hidden_layer_size,dropout_prob,batch_norm_lengths,num_of_hidden_layers=3, image_size=64*64*3):\n",
        "        \"\"\"\n",
        "        constructor\n",
        "        :param input_size: the input size\n",
        "        :param num_classes: the number of classes\n",
        "        :param hidden_layer_size: for each hidden layer, we have a tuple of the size  (input_size, output_size)\n",
        "        :param num_of_hidden_layers: the number of hidden layers\n",
        "        :param dropout_prob: the dropout probability for each layer\n",
        "        :param batch_norm_lengths: the length of each batch normalization layer\n",
        "        \"\"\"\n",
        "        super(ANN, self).__init__()\n",
        "        #flatten the image to a vector\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        #batch normalization\n",
        "        self.batch_norm_layers = nn.ModuleList()\n",
        "        #create the batch normalization layers\n",
        "        for i in range(len(batch_norm_lengths)):\n",
        "            self.batch_norm_layers.append(nn.BatchNorm1d(batch_norm_lengths[i]))\n",
        "\n",
        "        #dropout\n",
        "        self.dropout_list = nn.ModuleList()\n",
        "        #create the dropout layers\n",
        "        for i in range(num_of_hidden_layers):\n",
        "            self.dropout_list.append(nn.Dropout(p=dropout_prob[i]))\n",
        "\n",
        "        #input layer\n",
        "        self.input = nn.Linear(image_size, hidden_layer_size[0][0])\n",
        "\n",
        "        #create a list of hidden layers\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "\n",
        "        #create the hidden layers\n",
        "        for i in range(num_of_hidden_layers):\n",
        "            self.hidden_layers.append(nn.Linear(hidden_layer_size[i][0], hidden_layer_size[i][1]))\n",
        "\n",
        "        #output layer\n",
        "        self.output = nn.Linear(hidden_layer_size[-1][1], num_of_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward function\n",
        "        :param x: the input\n",
        "        :return: the output (predictions)\n",
        "        \"\"\"\n",
        "        #flatten the image\n",
        "        x = self.flatten(x)\n",
        "        #pass through the input layer\n",
        "        x = self.input(x)\n",
        "        #pass through the activation function\n",
        "        x = torch.relu(x)\n",
        "        #pass through the hidden layers\n",
        "        for i in range(len(self.hidden_layers)):\n",
        "            x = self.hidden_layers[i](x)\n",
        "            x = self.batch_norm_layers[i](x)\n",
        "            x = torch.relu(x)\n",
        "            x = self.dropout_list[i](x)\n",
        "\n",
        "        #pass through the output layer\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN**"
      ],
      "metadata": {
        "id": "PamGnAC051yx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9xpYae-Kurn_"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self,dropout_prob,batch_norm_lengths,conv_size,kernel_pooling_size,linear_size, num_of_classes=10,num_of_conv_layers=2,num_of_linear_layers=2):\n",
        "        \"\"\"\n",
        "        constructor\n",
        "        :droupout_prob: the dropout probability for each layer\n",
        "        :batch_norm_lengths: the length of each batch normalization layer\n",
        "        :conv_size: the params of each convolutional layers\n",
        "        :kernel_pooling_size: the size of the pooling kernel\n",
        "        :linear_size: the size of the linear layers\n",
        "        :num_of_classes: the number of classes\n",
        "        :num_of_conv_layers: the number of convolutional layers\n",
        "        :num_of_linear_layers: the number of linear layers\n",
        "        \"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        #convolutional layers\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        self.pooling_layers = nn.ModuleList()\n",
        "        #create the convolutional layers\n",
        "        for i in range(num_of_conv_layers):\n",
        "            #add the convolutional layer\n",
        "            self.conv_layers.append(nn.Conv2d(conv_size[i][0], conv_size[i][1], conv_size[i][2],conv_size[i][3],conv_size[i][4]))\n",
        "            #add the pooling layer\n",
        "            self.pooling_layers.append(nn.MaxPool2d(kernel_pooling_size[i]))\n",
        "\n",
        "       #batch normalization for the convolutional layers\n",
        "        self.batch_norm_layers = nn.ModuleList([nn.BatchNorm2d(batch_norm_lengths[i]) for i in range(len(batch_norm_lengths))])\n",
        "\n",
        "        #linear layers\n",
        "        #flatten the image\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        #dropout\n",
        "        self.dropout_list = nn.ModuleList([nn.Dropout(p=dropout_prob[i]) for i in range(num_of_linear_layers)])\n",
        "        #create the linear layers\n",
        "        self.linear1=nn.Linear(linear_size[0][0], linear_size[0][1])\n",
        "        self.linear2=nn.Linear(linear_size[1][0], linear_size[1][1])\n",
        "        self.output=nn.Linear(linear_size[1][1], num_of_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward function\n",
        "        :param x: the input\n",
        "        :return: the output (predictions)\n",
        "        \"\"\"\n",
        "        #pass through the convolutional layers\n",
        "        for i in range(len(self.conv_layers)):\n",
        "            x = self.conv_layers[i](x)\n",
        "            x = self.batch_norm_layers[i](x)\n",
        "            x = self.pooling_layers[i](x)\n",
        "            x = torch.relu(x)\n",
        "\n",
        "        #flatten the image\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        #pass through the linear layers\n",
        "        x = self.linear1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout_list[0](x)\n",
        "        x = self.linear2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout_list[1](x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def output_channels(input_size, kernel_size, padding, stride):\n",
        "        \"\"\"\n",
        "        calculate the output size of the convolutional layer\n",
        "        :param input_size: the input size\n",
        "        :param kernel_size: the kernel size\n",
        "        :param padding: the padding size\n",
        "        :param stride: the stride size\n",
        "        :return: the output size\n",
        "        \"\"\"\n",
        "        return int((input_size - kernel_size + 2 * padding) / stride) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **fixed pre-trained MobileNetV2**"
      ],
      "metadata": {
        "id": "_E1r9kWp6Gau"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DDp5k_Sww2P3"
      },
      "outputs": [],
      "source": [
        "class mobileNetV2(nn.Module):\n",
        "    def __init__(self,linear_size,dropout_prob,num_of_linear_layer=2, num_of_classes=10):\n",
        "        \"\"\"\n",
        "        constructor\n",
        "        :param num_of_classes: the number of classes\n",
        "        :param linear_size: the size of the linear layers (input_size, output_size)\n",
        "        :param dropout_prob: the dropout probability for each layer\n",
        "        :param num_of_linear_layer: the number of linear layers in the model not including the output layer\n",
        "\n",
        "        \"\"\"\n",
        "        super(mobileNetV2, self).__init__()\n",
        "        #load the mobilenetv2 model\n",
        "        self.model = models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n",
        "\n",
        "        #dont change the weights of the model\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        #create the linear layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.ModuleList([nn.Linear(size[0], size[1]) for size in linear_size ])\n",
        "        self.output=nn.Linear(linear_size[-1][1], num_of_classes)\n",
        "\n",
        "        #dropout\n",
        "        self.dropout_list = nn.ModuleList([nn.Dropout(p=dropout_prob[i]) for i in range(num_of_linear_layer)])\n",
        "\n",
        "        #batch normalization\n",
        "        self.batch_norm_layers = nn.ModuleList([nn.BatchNorm1d(size[1]) for size in linear_size])\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward function\n",
        "        :param x: the input\n",
        "        :return: the output (predictions)\n",
        "        \"\"\"\n",
        "        #pass through the mobilenetv2 model\n",
        "        x = self.model(x)\n",
        "        #flatten the image\n",
        "        x = self.flatten(x)\n",
        "        #pass through the linear layers\n",
        "        for i in range(len(self.linear)):\n",
        "            x = self.linear[i](x)\n",
        "            x = self.batch_norm_layers[i](x)\n",
        "            x = torch.relu(x)\n",
        "            x = self.dropout_list[i](x)\n",
        "        #pass through the output layer\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def clac_input_size():\n",
        "        \"\"\"\n",
        "        calculate the input size of the first linear layer\n",
        "        :return: the input size\n",
        "        \"\"\"\n",
        "        return models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).classifier[1].out_features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **learned pre-trained MobileNetV2**"
      ],
      "metadata": {
        "id": "F82paq9a6NZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "r2EhMmCHJPiY"
      },
      "outputs": [],
      "source": [
        "class mobileNetV2Learned(nn.Module):\n",
        "    def __init__(self, linear_size, dropout_prob, num_of_linear_layer=2, num_of_classes=10):\n",
        "        \"\"\"\n",
        "        constructor\n",
        "        :param num_of_classes: the number of classes\n",
        "        :param linear_size: the size of the linear layers (input_size, output_size)\n",
        "        :param dropout_prob: the dropout probability for each layer\n",
        "        :param num_of_linear_layer: the number of linear layers in the model not including the output layer\n",
        "\n",
        "        \"\"\"\n",
        "        super(mobileNetV2Learned, self).__init__()\n",
        "        # load the mobilenetv2 model\n",
        "        self.model = models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n",
        "\n",
        "        # change the weights of the model\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "        # create the linear layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.ModuleList([nn.Linear(size[0], size[1]) for size in linear_size])\n",
        "        self.output = nn.Linear(linear_size[-1][1], num_of_classes)\n",
        "\n",
        "        # dropout\n",
        "        self.dropout_list = nn.ModuleList([nn.Dropout(p=dropout_prob[i]) for i in range(num_of_linear_layer)])\n",
        "\n",
        "        # batch normalization\n",
        "        self.batch_norm_layers = nn.ModuleList([nn.BatchNorm1d(size[1]) for size in linear_size])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward function\n",
        "        :param x: the input\n",
        "        :return: the output (predictions)\n",
        "        \"\"\"\n",
        "        # pass through the mobilenetv2 model\n",
        "        x = self.model(x)\n",
        "        # flatten the image\n",
        "        x = self.flatten(x)\n",
        "        # pass through the linear layers\n",
        "        for i in range(len(self.linear)):\n",
        "            x = self.linear[i](x)\n",
        "            x = self.batch_norm_layers[i](x)\n",
        "            x = torch.relu(x)\n",
        "            x = self.dropout_list[i](x)\n",
        "        # pass through the output layer\n",
        "        x = self.output(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def clac_input_size():\n",
        "        \"\"\"\n",
        "        calculate the input size of the first linear layer\n",
        "        :return: the input size\n",
        "        \"\"\"\n",
        "        return models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).classifier[1].out_features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **test function**"
      ],
      "metadata": {
        "id": "plz4G80p6WwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader,loss_function):\n",
        "    \"\"\"\n",
        "    this function will test the model\n",
        "    :param model: the model we want to test\n",
        "    :param test_loader: the data loader of the testset\n",
        "    :return: the lost and the accuracy of the model during the test set\n",
        "    \"\"\"\n",
        "    #set the model to evaluation mode\n",
        "    model.eval()\n",
        "    #dont calculate the gradients\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_accuracy = 0\n",
        "        count = 0\n",
        "        for X_test, y_test in test_loader:\n",
        "            # move the testset to the device\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "            # first do a forward pass to get the predictions\n",
        "            y_hat_test = model(X_test)\n",
        "            # calculate the loss of the testset\n",
        "            loss_test = loss_function(y_hat_test, y_test)\n",
        "            total_loss += loss_test.item()\n",
        "            # calculate the accuracy of the testset\n",
        "            accuracy_test = torch.mean((y_test == torch.argmax(y_hat_test, axis=1)).float()).item()\n",
        "            total_accuracy += accuracy_test\n",
        "            count += 1\n",
        "\n",
        "        average_loss = total_loss / count\n",
        "        average_accuracy = total_accuracy / count\n",
        "\n",
        "    #print the results of the test\n",
        "    print(f'Test Loss: {average_loss:.3f} | '\n",
        "          f'Test Accuracy: {100 * average_accuracy:.3f}%')"
      ],
      "metadata": {
        "id": "Lb0IfSfuI0P7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **main function - run the different models**"
      ],
      "metadata": {
        "id": "2JsNhbVC6bRG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G58-xJ0YuHSS"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    main function\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    global batch_size\n",
        "    #get the trainloader and testloader\n",
        "    trainloader, testloader,visulaztion_loader,transform_train, transform_test = load_data()\n",
        "\n",
        "    #visualize the data\n",
        "    visualize_data(visulaztion_loader.dataset)\n",
        "\n",
        "    #show the agumentation\n",
        "    show_agumentation(visulaztion_loader.dataset)\n",
        "\n",
        "    #split the trainset to trainset and valset\n",
        "    trainset, valset = split_train_val(trainloader.dataset)\n",
        "\n",
        "    #apply the transformation on the trainset and the valset\n",
        "    #the transformation will be different for the trainset and the valset because we dont want to apply agumentation on the valset\n",
        "    trainset.transform = transform_train\n",
        "    valset.transform = transform_test\n",
        "\n",
        "    # create the dataloaders for the trainset and valset\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
        "    valloader = DataLoader(valset, batch_size=valset.data.shape[0], shuffle=False)\n",
        "\n",
        "\n",
        "    #~~~~~~~~~~~logistic regression~~~~~~~~~~~~~~~~~~~~~~\n",
        "    flatten_model_weight_decay=0.005\n",
        "    flatten_model_lr=0.001\n",
        "    flatten_model_num_of_epochs=20\n",
        "\n",
        "    model_logistic = LogisticRegression(10)\n",
        "\n",
        "    # create the optimizer\n",
        "    # optimizer type\n",
        "    if optimizer_type == 'SGD':\n",
        "        optimizer_logistic  = optim.SGD(model_logistic.parameters(), lr=flatten_model_lr, momentum=momentum, weight_decay=flatten_model_weight_decay)\n",
        "    elif optimizer_type == 'Adam':\n",
        "        optimizer_logistic  = optim.Adam( model_logistic.parameters(), lr=flatten_model_lr, betas=(0.9, 0.999), weight_decay=flatten_model_weight_decay)\n",
        "    elif optimizer_type == 'RMSProp':\n",
        "        optimizer_logistic  = optim.RMSprop( model_logistic.parameters(), lr=flatten_model_lr, alpha=0.99, eps=1e-08, weight_decay=flatten_model_weight_decay)\n",
        "    else:\n",
        "        NotImplementedError(\"optimizer not implemented\")\n",
        "\n",
        "\n",
        "\n",
        "    #create the loss function\n",
        "    loss_func_logistic=nn.CrossEntropyLoss()\n",
        "\n",
        "    #train the model\n",
        "    loss_train_logitic, loss_valdition_logistic, accuracy_train_logistic, accuracy_validtion_logitic ,model_logistic = \\\n",
        "        (train(model_logistic,trainloader,valloader,loss_func_logistic,optimizer_logistic,flatten_model_num_of_epochs))\n",
        "\n",
        "    #show the plots\n",
        "    show_plots(loss_train_logitic,loss_valdition_logistic,accuracy_train_logistic,accuracy_validtion_logitic)\n",
        "\n",
        "    #test the model\n",
        "    test(model_logistic,testloader,loss_func_logistic)\n",
        "\n",
        "    #~~~~~~~~~~~ANN~~~~~~~~~~~~~~~~~~~~~~\n",
        "    ANN_lr=0.001\n",
        "    ANN_weight_decay=0.001\n",
        "    num_of_epochs_ANN=34\n",
        "    model_ANN = ANN(10,hidden_layer_size=[(2048,1024),(1024,512),(512,256),(256,128)],dropout_prob=[0.1,0.08,0.05,0.03],batch_norm_lengths=[1024,512,256,128],num_of_hidden_layers=4)\n",
        "\n",
        "\n",
        "    # create the optimizer\n",
        "    # optimizer type\n",
        "    if optimizer_type == 'SGD':\n",
        "        optimizer_ANN  = optim.SGD(model_ANN.parameters(), lr=ANN_lr, momentum=momentum, weight_decay=ANN_weight_decay)\n",
        "    elif optimizer_type == 'Adam':\n",
        "        optimizer_ANN  = optim.Adam(model_ANN.parameters(), lr=ANN_lr, betas=(0.9, 0.999), weight_decay=ANN_weight_decay)\n",
        "    elif optimizer_type == 'RMSProp':\n",
        "        optimizer_ANN  = optim.RMSprop(model_ANN.parameters(), lr=ANN_lr, alpha=0.99, eps=1e-08, weight_decay=ANN_weight_decay)\n",
        "    else:\n",
        "        NotImplementedError(\"optimizer not implemented\")\n",
        "\n",
        "\n",
        "    #create the loss function\n",
        "    loss_func_ANN = nn.CrossEntropyLoss()\n",
        "    #loss_func_ANN=nn.NLLLoss()\n",
        "\n",
        "    #train the model\n",
        "    loss_train_ANN, loss_valdition_ANN, accuracy_train_ANN, accuracy_validtion_ANN ,model_ANN = \\\n",
        "        (train(model_ANN,trainloader,valloader,loss_func_ANN,optimizer_ANN,num_of_epochs_ANN))\n",
        "\n",
        "    #show the plots\n",
        "    show_plots(loss_train_ANN,loss_valdition_ANN,accuracy_train_ANN,accuracy_validtion_ANN)\n",
        "\n",
        "    #test the model\n",
        "    test(model_ANN,testloader,loss_func_ANN)\n",
        "\n",
        "   #check how many parameters the model has\n",
        "    print(summary(model_ANN, (3, 64, 64)))\n",
        "\n",
        "    #~~~~~~~~~~~CNN~~~~~~~~~~~~~~~~~~~~~~\n",
        "    CNN_lr=0.01\n",
        "    CNN_weight_decay=0.001\n",
        "    num_of_epochs_CNN=40\n",
        "    CNN_opitmize_type='RMSProp'\n",
        "    #set the number of conv layers and linear layers\n",
        "    conv_layers = 3\n",
        "    hidden_linear_layers = 2\n",
        "    #for each conv layer we define the size of the input and output channels,the kernel size ,the stride and the padding\n",
        "    conv_sizes = [[3,12,3,1,0],[12,12,3,1,0],[12,42,3,1,0]]\n",
        "    #set the size of the kernel pooling\n",
        "    kernel_pooling_size = [2, 4, 2]\n",
        "\n",
        "    #clculate the size of the input of the first linear layer\n",
        "    input_linear_size=64\n",
        "    for i in range(conv_layers):\n",
        "        #calculate the size after the convolutional layer\n",
        "        input_linear_size = CNN.output_channels(input_linear_size,conv_sizes[i][2],conv_sizes[i][4],conv_sizes[i][3])\n",
        "        #calculate the size after the pooling layer\n",
        "        input_linear_size = input_linear_size // kernel_pooling_size[i]\n",
        "\n",
        "    #before the first linear layer we need to flatten the image,the image will be in the shape of(input_linear_size,input_linear_size,cov_sizes[-1][1])\n",
        "    input_linear_size = input_linear_size * input_linear_size * conv_sizes[-1][1]\n",
        "\n",
        "\n",
        "\n",
        "    model_CNN = CNN(dropout_prob=[0.03,0.01],batch_norm_lengths=[12,12,42],conv_size=conv_sizes,kernel_pooling_size=kernel_pooling_size,linear_size=[(input_linear_size,200),(200,100)],num_of_conv_layers=3)\n",
        "\n",
        "    #check how many parameters the model has\n",
        "    print(summary(model_CNN, (3, 64, 64)))\n",
        "\n",
        "    # create the optimizer\n",
        "    # optimizer type\n",
        "    if  CNN_opitmize_type == 'SGD':\n",
        "        optimizer_CNN  = optim.SGD(model_CNN.parameters(), lr=CNN_lr, momentum=momentum, weight_decay=CNN_weight_decay)\n",
        "    elif optimizer_type == 'Adam':\n",
        "        optimizer_CNN  = optim.Adam(model_CNN.parameters(), lr=CNN_lr, betas=(0.9, 0.999), weight_decay=CNN_weight_decay)\n",
        "    elif optimizer_type == 'RMSProp':\n",
        "        optimizer_CNN  = optim.RMSprop(model_CNN.parameters(), lr=CNN_lr, alpha=0.99, eps=1e-08, weight_decay=CNN_weight_decay)\n",
        "    else:\n",
        "        NotImplementedError(\"optimizer not implemented\")\n",
        "\n",
        "\n",
        "    #create the loss function\n",
        "    loss_func_CNN = nn.CrossEntropyLoss()\n",
        "\n",
        "    #train the model\n",
        "    loss_train_CNN, loss_valdition_CNN, accuracy_train_CNN, accuracy_validtion_CNN ,model_CNN = \\\n",
        "        (train(model_CNN,trainloader,valloader,loss_func_CNN,optimizer_CNN, num_of_epochs_CNN))\n",
        "\n",
        "    #show the plots\n",
        "    show_plots(loss_train_CNN,loss_valdition_CNN,accuracy_train_CNN,accuracy_validtion_CNN)\n",
        "\n",
        "    #test the model\n",
        "    test(model_CNN,testloader,loss_func_CNN)\n",
        "\n",
        "    # #~~~~~~~~~~~mobile net v2~~~~~~~~~~~~~~~~~~~~~~\n",
        "    mobileNet_lr=0.001\n",
        "    mobileNet_weight_decay=0.001\n",
        "    num_of_epochs_mobileNet=30\n",
        "    # get the size of the input of the first linear layer\n",
        "    input_linear_size = mobileNetV2.clac_input_size()\n",
        "\n",
        "    model_v2 = mobileNetV2(linear_size=[(input_linear_size,200),(200,100)],dropout_prob=[0.08,0.03])\n",
        "\n",
        "    # create the optimizer\n",
        "    # optimizer type\n",
        "    if optimizer_type == 'SGD':\n",
        "        optimizer_v2 = optim.SGD(model_v2.parameters(), lr=mobileNet_lr, momentum=momentum, weight_decay= mobileNet_weight_decay)\n",
        "    elif optimizer_type == 'Adam':\n",
        "        optimizer_v2 = optim.Adam(model_v2.parameters(), lr=mobileNet_lr, betas=(0.9, 0.999), weight_decay= mobileNet_weight_decay)\n",
        "    elif optimizer_type == 'RMSProp':\n",
        "        optimizer_v2 = optim.RMSprop(model_v2.parameters(), lr=mobileNet_lr, alpha=0.99, eps=1e-08, weight_decay= mobileNet_weight_decay)\n",
        "    else:\n",
        "        NotImplementedError(\"optimizer not implemented\")\n",
        "\n",
        "    #optimizer_v2 = optim.Adam(model_v2.parameters(), lr=0.001,weight_decay=weight_decay)\n",
        "\n",
        "    #create the loss function\n",
        "    loss_func_v2 = nn.CrossEntropyLoss()\n",
        "\n",
        "    #train the model\n",
        "    loss_train_v2, loss_valdition_v2, accuracy_train_v2, accuracy_validtion_v2 ,model_v2 = \\\n",
        "        (train(model_v2,trainloader,valloader,loss_func_v2,optimizer_v2,num_of_epochs_mobileNet))\n",
        "\n",
        "    #show the plots\n",
        "    show_plots(loss_train_v2,loss_valdition_v2,accuracy_train_v2,accuracy_validtion_v2)\n",
        "\n",
        "    #test the model\n",
        "    test(model_v2,testloader,loss_func_v2)\n",
        "\n",
        "    #~~~~~~~~~~~~~~mobilenet v2 learning weights ~~~~~~~~~~~~~~~~~~\n",
        "    mobileNetLearning_lr=0.0005\n",
        "    mobileNetLearning_weight_decay=0.0005\n",
        "    num_of_epochs_mobileNetLearning=20\n",
        "    # get the size of the input of the first linear layer\n",
        "    input_linear_size = mobileNetV2Learned.clac_input_size()\n",
        "\n",
        "    model_v2_learning_weights = mobileNetV2Learned(linear_size=[(input_linear_size,200),(200,50)],dropout_prob=[0.08,0.01])\n",
        "\n",
        "    # # create the optimizer\n",
        "    # optimizer_v2_learning_weights = optim.Adam(model_v2_learning_weights.parameters(), lr=lr)\n",
        "    # create the optimizer\n",
        "    # optimizer type\n",
        "    if optimizer_type == 'SGD':\n",
        "        optimizer_v2_learning_weights  = optim.SGD(model_v2_learning_weights.parameters(), lr= mobileNetLearning_lr, momentum=momentum, weight_decay=mobileNetLearning_weight_decay)\n",
        "    elif optimizer_type == 'Adam':\n",
        "        optimizer_v2_learning_weights  = optim.Adam(model_v2_learning_weights.parameters(), lr= mobileNetLearning_lr, betas=(0.9, 0.999), weight_decay=mobileNetLearning_weight_decay)\n",
        "    elif optimizer_type == 'RMSProp':\n",
        "       optimizer_v2_learning_weights  = optim.RMSprop(model_v2_learning_weights.parameters(), lr= mobileNetLearning_lr, alpha=0.99, eps=1e-08, weight_decay=mobileNetLearning_weight_decay)\n",
        "    else:\n",
        "        NotImplementedError(\"optimizer not implemented\")\n",
        "\n",
        "    #create the loss function\n",
        "    loss_func_v2_learning_weights = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    #train the model\n",
        "    loss_train_v2_learning_weights, loss_valdition_v2_learning_weights, accuracy_train_v2_learning_weights, accuracy_validtion_v2_learning_weights ,model_v2_learning_weights = \\\n",
        "        (train(model_v2_learning_weights,trainloader,valloader,loss_func_v2_learning_weights,optimizer_v2_learning_weights, num_of_epochs_mobileNetLearning))\n",
        "\n",
        "    #show the plots\n",
        "    show_plots(loss_train_v2_learning_weights,loss_valdition_v2_learning_weights,accuracy_train_v2_learning_weights,accuracy_validtion_v2_learning_weights)\n",
        "\n",
        "    # #test the model\n",
        "    test(model_v2_learning_weights,testloader,loss_func_v2_learning_weights)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}